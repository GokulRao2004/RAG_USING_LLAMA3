{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d53c97b",
   "metadata": {},
   "source": [
    "# RAG using Ollama and Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528879e",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of this project is to develop a Retrieval-Augmented Generation (RAG) model that provides assistance based on user queries. The key components and goals of this project are as follows:\n",
    "\n",
    "1. **Integration of LangChain**:\n",
    "   - Utilize LangChain, a powerful framework designed for building language model applications, to streamline the development process.\n",
    "   - Leverage its document loaders, text splitters, and retrievers to efficiently manage and process documents.\n",
    "\n",
    "2. **Embedding Generation**:\n",
    "   - Employ high-quality embeddings using Ollama's implementation of LLaMA 3 to capture the semantic meaning of texts.\n",
    "   - Use the `OllamaEmbeddings` model to generate embeddings that enhance the retrieval accuracy of relevant documents.\n",
    "\n",
    "3. **Contextual Response Generation**:\n",
    "   - Enable the model to generate context-aware responses by utilizing the capabilities of LLaMA 3 integrated with Ollama.\n",
    "   - Create dynamic prompts that guide the model in generating accurate information based on user input.\n",
    "\n",
    "4. **Real-Time Information Retrieval**:\n",
    "   - Implement an efficient retrieval mechanism that allows users to access relevant documents and information in real time.\n",
    "   - Use a multi-query retriever from LangChain to generate variations of user queries, improving the likelihood of finding pertinent information.\n",
    "\n",
    "5. **User-Friendly Interface**:\n",
    "   - Develop an intuitive user interface using Gradio, allowing users to easily input their queries and receive prompt, relevant responses.\n",
    "   - Ensure the interface is accessible and responsive, enhancing the overall user experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf379ab",
   "metadata": {},
   "source": [
    "## Installations\n",
    "\n",
    "To set up the RAG model, the following libraries and tools are required:\n",
    "\n",
    "1. **Python**: Ensure Python (version 3.7 or later) is installed on your system.\n",
    "\n",
    "2. **PyTorch**: PyTorch is an open-source machine learning library used for applications such as computer vision and natural language processing. It provides a flexible framework for building deep learning models and is particularly known for its ease of use and dynamic computation graph.\n",
    "   - Install PyTorch based on availability of NVIDIA GPU and CUDA compatibility.\n",
    "\n",
    "3. **Ollama**: \n",
    "   - Ollama provides an easy way to run and manage large language models, including LLaMA 3. Ensure you follow the [installation instructions from the Ollama website](https://ollama.com/download) to get started.\n",
    "   - Pull the required models from ollama.\n",
    "\n",
    "4. **LangChain**: A library for building language model applications.\n",
    "\n",
    "5. **LangChain Community**: For additional functionality and extensions.\n",
    "\n",
    "6. **Unstructured**: A library for processing unstructured data.\n",
    "   - Includes various document loaders for processing different types of documents.\n",
    "\n",
    "7. **Chroma Vector Store**: For managing the vector embeddings.\n",
    "\n",
    "8. **LangChain Text Splitters**: For splitting text into manageable chunks.\n",
    "\n",
    "9. **Other Required Libraries**: Ensure any additional libraries needed for specific functionalities are also installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4f493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_community unstructured unstructured[all-docs] langchain chromadb langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c35be2",
   "metadata": {},
   "source": [
    "### Installation of Torch for CPU only System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01972b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245cdbd",
   "metadata": {},
   "source": [
    "### Installation of Torch for CUDA enabled GPU systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3811e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace cu124 with your CUDA version\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef74c45",
   "metadata": {},
   "source": [
    "### Pulling required models from Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd11664",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3 \n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0baf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a26ea",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "The following steps outline the procedure to set up and run the RAG model:\n",
    "\n",
    "1. **Load the PDF Document**:\n",
    "   - Use `UnstructuredPDFLoader` to load the legal document from which the model will retrieve information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82066bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LegalBot\\RAG_USING_OLLAMA\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "loader = UnstructuredPDFLoader(file_path=\"MotorACT.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618a4ed",
   "metadata": {},
   "source": [
    "2. **Split the Text into Chunks**:\n",
    "   - Use `RecursiveCharacterTextSplitter` to divide the loaded document into manageable chunks for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73233642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340da244",
   "metadata": {},
   "source": [
    "3. **Set Up the Vector Store**:\n",
    "   - Create a vector store using Chroma to hold the embeddings of the document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0493cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing Chroma vector store.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n",
    "current_dir = os.getcwd()\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_for_MotorAct\")\n",
    "\n",
    "if os.path.exists(persistent_directory):\n",
    "    vector_db = Chroma(\n",
    "        persist_directory=persistent_directory, \n",
    "        embedding_function=embedding_function,\n",
    "        collection_name=\"local-rag\"\n",
    "    )\n",
    "    print(\"Loaded existing Chroma vector store.\")\n",
    "else:\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks, \n",
    "        embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "        collection_name=\"local-rag\",\n",
    "        persist_directory=persistent_directory\n",
    "    )\n",
    "    vector_db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b5bf0",
   "metadata": {},
   "source": [
    "4. **Define the Prompt and Model**:\n",
    "   - Create prompts for generating responses based on user queries and initialize the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c9d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", show_progress=True)\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI legal assistant. Your task is to generate five\n",
    "    different versions of the given legal question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a03c3",
   "metadata": {},
   "source": [
    "5. **Set Up the Retriever**:\n",
    "   - Use a multi-query retriever to enhance the retrieval process, generating variations of user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d446b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(),\n",
    "    llm=llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f16490",
   "metadata": {},
   "source": [
    "6. **Defining the Prompt Template**:\n",
    "   - Create a prompt template for the AI legal assistant to ensure it answers user questions based solely on the provided legal context, generating accurate and context-specific responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c161b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI legal assistant. Answer the question based ONLY on the following legal context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402d933",
   "metadata": {},
   "source": [
    "7. **Function to Retrieve Answers**:\n",
    "   - This function retrieves answers from the AI legal assistant based on the user's question, utilizing a defined prompt and retrieval chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a299124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def get_answer(question):\n",
    "    if not question:\n",
    "        return \"Please enter a question.\"\n",
    "\n",
    "    try:\n",
    "        # Create the chain\n",
    "        chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        answer = chain.invoke(question)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4c38b",
   "metadata": {},
   "source": [
    "8. **Create the Gradio Interface**:\n",
    "   - Develop a user-friendly interface using Gradio to allow users to input queries and receive responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da91198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "with gr.Blocks() as iface:\n",
    "    gr.Markdown(\"<h1 style='text-align: center;'>Legal Assistant Chatbot</h1>\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(label=\"Enter your legal question:\", lines=2, placeholder=\"Type your question here...\")\n",
    "        \n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "    answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
    "    \n",
    "    # Define what happens when the button is clicked\n",
    "    submit_button.click(get_answer, inputs=question_input, outputs=answer_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c06e1265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:03<00:00,  4.00s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
